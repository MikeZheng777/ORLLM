{
  "os":  "Linux-4.18.0-372.9.1.el8.x86_64-x86_64-with-glibc2.28",
  "python":  "CPython 3.11.14",
  "startedAt":  "2025-12-10T13:58:52.404675Z",
  "args":  [
    "--model_name_or_path",
    "/home/hanzheng/orcd/scratch/models/Qwen3-8B",
    "--train_dataset_name_or_path",
    "train_test_data/converted_data_completion_new_200.json",
    "--output_dir",
    "/home/hanzheng/orcd/scratch/orlm_finetune/experiments/data_size_200",
    "--per_device_train_batch_size",
    "2",
    "--per_device_eval_batch_size",
    "2",
    "--gradient_accumulation_steps",
    "4",
    "--eval_strategy",
    "no",
    "--eval_steps",
    "10000",
    "--save_strategy",
    "no",
    "--save_steps",
    "999999",
    "--save_total_limit",
    "0",
    "--save_on_each_node",
    "False",
    "--load_best_model_at_end",
    "False",
    "--preprocessing_num_workers",
    "0",
    "--ddp_timeout",
    "14400",
    "--max_seq_length",
    "8192",
    "--learning_rate",
    "2e-5",
    "--lr_scheduler_type",
    "linear",
    "--warmup_ratio",
    "0.03",
    "--logging_steps",
    "5",
    "--report_to",
    "wandb",
    "--run_name",
    "data_size_200",
    "--gradient_checkpointing",
    "True",
    "--deepspeed",
    "train/configs/h200_optimized_bf16.json",
    "--overwrite_output_dir",
    "--bf16",
    "True",
    "--use_lora",
    "false",
    "--lora_rank",
    "16",
    "--lora_alpha",
    "16",
    "--use_auth_token",
    "True",
    "--remove_unused_columns",
    "False",
    "--num_train_epochs",
    "4"
  ],
  "program":  "-m train.finetune",
  "git":  {
    "remote":  "git@github.com:MikeZheng777/RLLM_OPT.git",
    "commit":  "0fba54fff6d09134ffdf39c60511c2eb0d14fa7c"
  },
  "email":  "hanzheng@mit.edu",
  "root":  "/orcd/home/002/hanzheng/projects/RLLM_OPT",
  "host":  "node3300",
  "executable":  "/home/hanzheng/.conda/envs/orlm/bin/python3.11",
  "cpu_count":  120,
  "cpu_count_logical":  240,
  "gpu":  "NVIDIA H200",
  "gpu_count":  2,
  "disk":  {
    "/":  {
      "total":  "944364355584",
      "used":  "26390216704"
    }
  },
  "memory":  {
    "total":  "2163435831296"
  },
  "gpu_nvidia":  [
    {
      "name":  "NVIDIA H200",
      "memoryTotal":  "150754820096",
      "cudaCores":  16896,
      "architecture":  "Hopper",
      "uuid":  "GPU-c78988be-d375-c5af-cdc7-1ff633887e66"
    },
    {
      "name":  "NVIDIA H200",
      "memoryTotal":  "150754820096",
      "cudaCores":  16896,
      "architecture":  "Hopper",
      "uuid":  "GPU-c8379491-8104-d6bc-fc4f-86d3af9554bf"
    }
  ],
  "cudaVersion":  "12.4",
  "slurm":  {
    "cluster_name":  "eofe7",
    "conf":  "/etc/slurm/slurm.conf",
    "cpus_on_node":  "1",
    "gpus_on_node":  "2",
    "gtids":  "0",
    "job_account":  "mit_general",
    "job_cpus_per_node":  "1",
    "job_end_time":  "1765381908",
    "job_gid":  "100191637",
    "job_gpus":  "0,1",
    "job_id":  "7093073",
    "job_name":  "interactive",
    "job_nodelist":  "node3300",
    "job_num_nodes":  "1",
    "job_partition":  "mit_normal_gpu",
    "job_qos":  "normal",
    "job_start_time":  "1765374708",
    "job_uid":  "191637",
    "job_user":  "hanzheng",
    "jobid":  "7093073",
    "launch_node_ipaddr":  "10.1.221.1",
    "localid":  "0",
    "mem_per_node":  "131072",
    "mpi_type":  "none",
    "nnodes":  "1",
    "nodeid":  "0",
    "nodelist":  "node3300",
    "oom_kill_step":  "0",
    "prio_process":  "0",
    "procid":  "0",
    "pty_port":  "41693",
    "pty_win_col":  "169",
    "pty_win_row":  "68",
    "script_context":  "prolog_task",
    "srun_comm_host":  "10.1.221.1",
    "srun_comm_port":  "46139",
    "step_id":  "4294967290",
    "step_launcher_port":  "46139",
    "step_nodelist":  "node3300",
    "step_num_nodes":  "1",
    "step_num_tasks":  "1",
    "step_tasks_per_node":  "1",
    "stepid":  "4294967290",
    "submit_dir":  "/orcd/home/002/hanzheng",
    "submit_host":  "orcd-login001.mit.edu",
    "task_pid":  "2485142",
    "tasks_per_node":  "1",
    "topology_addr":  "node3300",
    "topology_addr_pattern":  "node"
  },
  "writerId":  "u30mh4ooh26mziqe13hw8repzegmvm2m"
}