{
  "os":  "Linux-4.18.0-553.56.1.el8_10.x86_64-x86_64-with-glibc2.28",
  "python":  "CPython 3.11.14",
  "startedAt":  "2025-12-10T07:35:14.536767Z",
  "args":  [
    "--model_name_or_path",
    "/home/hanzheng/orcd/scratch/models/Qwen3-8B",
    "--train_dataset_name_or_path",
    "train_test_data/converted_data_completion_new_400.json",
    "--output_dir",
    "/home/hanzheng/orcd/scratch/orlm_finetune/experiments/lora_rank_64",
    "--per_device_train_batch_size",
    "2",
    "--per_device_eval_batch_size",
    "2",
    "--gradient_accumulation_steps",
    "4",
    "--eval_strategy",
    "no",
    "--eval_steps",
    "10000",
    "--save_strategy",
    "no",
    "--save_steps",
    "999999",
    "--save_total_limit",
    "0",
    "--save_on_each_node",
    "False",
    "--load_best_model_at_end",
    "False",
    "--preprocessing_num_workers",
    "0",
    "--ddp_timeout",
    "14400",
    "--max_seq_length",
    "8192",
    "--learning_rate",
    "0.0002",
    "--lr_scheduler_type",
    "linear",
    "--warmup_ratio",
    "0.03",
    "--logging_steps",
    "5",
    "--report_to",
    "wandb",
    "--run_name",
    "lora_rank_64",
    "--gradient_checkpointing",
    "False",
    "--deepspeed",
    "train/configs/h200_optimized_bf16_lora.json",
    "--overwrite_output_dir",
    "--bf16",
    "True",
    "--use_lora",
    "true",
    "--lora_rank",
    "64",
    "--lora_alpha",
    "64",
    "--use_auth_token",
    "True",
    "--remove_unused_columns",
    "False",
    "--num_train_epochs",
    "4"
  ],
  "program":  "-m train.finetune",
  "git":  {
    "remote":  "git@github.com:MikeZheng777/RLLM_OPT.git",
    "commit":  "f87e568a9b49163eb9fe5c05dddc92a2f88454c3"
  },
  "email":  "hanzheng@mit.edu",
  "root":  "/orcd/home/002/hanzheng/projects/RLLM_OPT",
  "host":  "node4100",
  "executable":  "/home/hanzheng/.conda/envs/orlm/bin/python3.11",
  "cpu_count":  120,
  "cpu_count_logical":  240,
  "gpu":  "NVIDIA H200",
  "gpu_count":  2,
  "disk":  {
    "/":  {
      "total":  "1081980358656",
      "used":  "10645131264"
    }
  },
  "memory":  {
    "total":  "2163960721408"
  },
  "gpu_nvidia":  [
    {
      "name":  "NVIDIA H200",
      "memoryTotal":  "150754820096",
      "cudaCores":  16896,
      "architecture":  "Hopper",
      "uuid":  "GPU-1f860fee-1172-7d14-fc3f-794f86ea56fe"
    },
    {
      "name":  "NVIDIA H200",
      "memoryTotal":  "150754820096",
      "cudaCores":  16896,
      "architecture":  "Hopper",
      "uuid":  "GPU-525a1731-fae7-4c9e-bc05-d177ceb47222"
    }
  ],
  "cudaVersion":  "13.0",
  "slurm":  {
    "cluster_name":  "eofe7",
    "conf":  "/etc/slurm/slurm.conf",
    "cpus_on_node":  "2",
    "gpus_on_node":  "2",
    "gtids":  "0",
    "job_account":  "mit_general",
    "job_cpus_per_node":  "2",
    "job_end_time":  "1765358087",
    "job_gid":  "100191637",
    "job_gpus":  "1,2",
    "job_id":  "7089625",
    "job_name":  "interactive",
    "job_nodelist":  "node4100",
    "job_num_nodes":  "1",
    "job_partition":  "mit_normal_gpu",
    "job_qos":  "normal",
    "job_start_time":  "1765350887",
    "job_uid":  "191637",
    "job_user":  "hanzheng",
    "jobid":  "7089625",
    "launch_node_ipaddr":  "10.1.221.1",
    "localid":  "0",
    "mem_per_node":  "131072",
    "mpi_type":  "none",
    "nnodes":  "1",
    "nodeid":  "0",
    "nodelist":  "node4100",
    "oom_kill_step":  "0",
    "prio_process":  "0",
    "procid":  "0",
    "pty_port":  "43505",
    "pty_win_col":  "169",
    "pty_win_row":  "72",
    "script_context":  "prolog_task",
    "srun_comm_host":  "10.1.221.1",
    "srun_comm_port":  "34925",
    "step_id":  "4294967290",
    "step_launcher_port":  "34925",
    "step_nodelist":  "node4100",
    "step_num_nodes":  "1",
    "step_num_tasks":  "1",
    "step_tasks_per_node":  "1",
    "stepid":  "4294967290",
    "submit_dir":  "/orcd/home/002/hanzheng",
    "submit_host":  "orcd-login001.mit.edu",
    "task_pid":  "350385",
    "tasks_per_node":  "2",
    "topology_addr":  "node4100",
    "topology_addr_pattern":  "node"
  },
  "writerId":  "3mpy1x5dvidvd8fhfvc6xkx58mfvji72"
}